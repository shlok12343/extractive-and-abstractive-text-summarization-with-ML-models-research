Markov Decision Processes

In our discussions thus far, we explored modeling dynamic environments that involve probabilistic events where outcomes are not certain but have associated transition (conditional) probabilities. We have also previously explored searches in deterministic environments, where actions lead to predictable outcomes. We now introduce stochastic environments to our search problems, where an action taken from a state is no longer guaranteed to lead to a specific successor state. Instead, we now consider the scenario where there is a probability associated with each action leading our agent from one state to another. Consider the following scenario:

Here, from state 

, our agent takes action 

, but may end up in either state 

 or state 

, with associated probabilities 

 and 

 respectively. In other words, 

 of the time, the agent will end up in 

, and 

 of the time in 

. This stochastic system can be formally represented as a Markov Decision Process (MDP), which is a mathematical model describing a problem characterized by uncertainty.

State Space, 

 : the set of all possible states that the system can be in. In the above example, 

.

Action Space, 

 : the set of all possible actions. In the above example, 

, since there is only one possible action from a state in 

. No actions can be taken from states 

 and 

, and are therefore terminal or end states in the MDP.

Transition Probabilities, 

 : a the probabilities of transitioning from one state to another given a specific action, notated as 

, where 

 is the source state, 

 is the action and 

 is the target/successor state. In our example, 

, and 

.

Rewards, 

 : a numerical reward associated with each transition. In general, 

 should be thought of as the reward of reaching target state 

 from state 

 using action 

.

Discount Factor, 

 : accounts for the discounting of future rewards in decision-making. It ensures that immediate rewards are valued more than future rewards. Discounting rewards in general prevents the agent from choosing actions that may lead to infinite rewards (cyclical paths).

Finally, a policy 

 is defined as a mapping from states to actions. In other words, a policy defines a choice of one action for every state in the MDP (we will adopt a different, probabilisitc notion of a policy when we get to reinforcement learning). Note that a policy does not concern itself with the transition probabilities or rewards, but only with the actions to be taken in each state. It also says nothing about a specific path the agent might end up taking as a result of the chosen actions. Different policies, therefore, may lead to different cumulative rewards on average. The goal of the agent in an MDP is to find the policy that maximizes the expected cumulative reward over time. This is known as the optimal policy. Consider, for instance, the Frodo example from class/assignment 2.

An example policy for the Frodo puzzle is shown in the figure on the right, where for each state, the policy fixes an action. We use X to denote terminal states where no action can be taken. The policy can equivalently be represented as 

. Here, each coordinate within a maze represents a state, and each state is associated with an action selected from the action-space 

. Note that while this policy may look like it leads us to the state with the maximum positive reward, whether this policy is optimal depends on the transition probabilities. If, for instance, the probability of Frodo slipping in a direction other than the direction of the action is sufficiently high, this policy may never lead us to the state with the maximum positive reward.

Evaluating Policies

In order to find the best policy for our agent, we must be able to compare two or more given policies quantitatively, to figure out which of the policies is expected to yield a higher return on average. Remember that under any one policy, every time the agent is in a state, it takes the same action; however, since the agent may end up in a different state each time (based on the values in 

), the outcome of the policy may vary (we can stop 'playing' when the agent reaches a terminal state). This is why we need to evaluate policies in terms of their expected cumulative rewards rather than what happens in any one specific sample. A single actual path taken by the agent as a result of following a policy is called an episode. We define the utility of an episode as the discounted sum of rewards along the path. Given the episode 

, the utility of the episode is given by:

where 

 is the reward at time 

, and 

 is the discount factor. The expected utility of a policy 

 is then given by the expected value of the utility of the episodes generated by the policy. Given a series of episodes, the expected value is simply the average of the utilities obtained in each episode; i.e., given a set of episodes 

, the expected utility of the policy is given by:

In practice, we are often concerned about the expected value of starting in a given state 

 and following a policy 

 from there. This is called the value of the state under the policy, and denoted 

. Here, it is useful to define a second, related quantity, called the Q-value, defined over a state-action pair. The Q-value of a state-action pair 

 under a policy 

, denoted 

, is the expected utility of starting in state 

, taking action 

, and then following policy 

 thereafter. Note that in the case where the action 

 is also the action dictated by the policy 

, then the value of the state 

 under 

 is equal to the Q-value for that state-action pair. In other words, 

. Finally, also note that the value of any end state is always 

, since no action can be taken from that state.

With this in mind, we now need to find a way to calculate 

. Recall that from state 

, the agent takes action 

, and with probability 

, it transitions to a successor state 

. From 

 the agent follows policy 

, and the process continues. However, the Markov assumption allows us to simplify this process to a recurrence relation, spanning one step at a time. We can deconstruct the expected utility of starting in state 

, taking action 

, and then following policy 

 as 1) ending up in one of several possible states 

, 2) receiving the immediate reward of the transition from 

 to 

 using 

, and 3) the expected utility of continuing from 

 and following policy 

 thereafter. We need to account for all possible candidates for 

 for a given 

 pair; however since only one of those candidates will be the actual successor state, we take the expected value of the utility of each candidate. This is given by the following Bellman equation:

When 

, then 

, and the value of the state under the policy 

 is then given by:

While in certain situations, the above equations may yield a closed-form solution for the value of a state under a policy, in general, we need to solve a system of linear equations to find the value of each state under the policy. This is done using a dynamic programming algorithm that iteratively computes the value of each state under the policy until the values converge. This algorithm is called the Policy Evaluation algorithm, and works as follows. We re-write the above equation to use previous estimates of 

 to compute new estimates of 

 as follows:

where 

 is the estimate of the value of state 

 under policy 

 at iteration 

. We start with an initial guess for the value of each state (usually set to 0), and then iteratively update the value of each state using the above equation until the values converge. Let us run through an example!

Consider the dice game example from class. The agent begins in a "start" state, and can take one of two actions: "stay" or "quit". If the agent chooses "stay", it receives a reward of $4, and then a die is rolled. If the die roll results in a number greater than 2, the agent gets to play again, but if the die results in 1 or 2, the game ends. Equivalently, we can say that 

. If the agent chooses "quit" from the "start" state, it receives a reward of $10, and the game ends; therefore 

. This game can be represented as the following MDP:

Let us now calculate the expected value of the start state, for a fixed "stay" policy, i.e., the agent always chooses to stay in the game. This is the type of policy evaluation that we are currently interested in - our goal is to find the best action for each state. Now, from the start state, given the policy 

, the Bellman equation can be expanded as follows:

Since the "stay" action can lead us to two possible states 

, we can expand the above equation as follows:

For simplicity, let us assume 

. We can now substitute the known values of the transition probabilities and rewards to get:

Now recall that the value of an end state is always 

, since no further actions can be taken from them. Therefore, 

. Substituting this value into the above equation, we get:

which simplifies to:

and finally to:

This is an example of a simple policy evaluation where the Bellman equation leads to a closed form solution. However this is rarely the case, and we usually need to solve a system of linear equations to find the value of each state under a policy. Let us consider the same MDP, but instead, use the iterative version of the Bellman update equation (the policy evaluation algorithm) to estimate 

 for all states.

We initialize the algorithm by setting 

 and 

. We then iteratively update the value of each state using the following equation:

This time, it is simply a matter of plugging in values for the transition probabilities and rewards, and our previous estimates for the 

 terms, and iterating until the values converge.

Assuming 

,

For the end state, 

 at all times. With these updated estimates, we can now compute 

 using the same equation as:

After a few more iterations, we find that the values converge to 

, and 

. This is the same value we obtained using the closed-form solution.

Finding the Best Policy

Now that we have a way to evaluate the value of each state under a policy, we can use this information to find the best policy. The best policy is the one that maximizes the value of each state. In other words, the best policy is the one that maximizes the expected cumulative reward over time. This is known as the optimal policy. Given the value of each state 

 under some policy 

, we can find the best action for each state by simply taking the action maximizes the expected value, i.e. 

. This is known as the greedy policy. We can use the previous policy evaluation algorithm to evaluate this greedy policy, and then find the best action for each state to form a new policy. We can then evaluate this new policy, and so on, until the policy converges. This is known as the policy iteration algorithm.

Let us now consider the same dice game example, but this time, find the best policy using the policy iteration algorithm. In this case, we are interested in the optimal value function 

, which is the value of each state under the optimal policy. We start with an initial guess for the value of each state (usually set to 0), and then iteratively update the value of each state using the policy evaluation algorithm until the values converge. However, there is one key difference - instead of using a fixed policy 

, we use the greedy policy at each iteration such that the chosen action maximizes the expected value. We start by setting 

 and 

. Note that 

 will always be 

, since it is an end state. We then iteratively update the value for the start state using the following equation:

Note that this equation computes the expected cumulative reward at the start state assuming the agent continues to follow the greedy policy.

Assuming 

,

Substituting known values for the transition probabilities and rewards, we get:

Of the two values within the max operator, the value corresponding to the 'quit' action was greater, therefore the optimal policy after the first round is 

. However, we know from our earlier closed-form solution that the expected value of always taking the "stay" action from the start state is 12, and therefore should be the optimal policy in the long run. Let us try one more update, to see if the values converge to a new policy.

Assuming 

 and plugging in our previous estimates,

Since the value inside the max operator corresponding to the 'stay' action is now greater, the optimal policy after the second round is 

. After a few more iterations, 

 converges to 

, and the optimal policy remains choosing the "stay" action at the start state.

Partially Observable MDPs

In our discussion of MDPs thus far, we have assumed that the agent has complete information about the world, in that the agent knows precisely which state in the state-space it is currently in. However, in many real-life implementations of autonomous agents, the agent does not have a global view of the world, but must instead rely on observations of its immediate surroundings. Using these observations, the agent may then compute how likely it is to be in every possible state in the state space. Such scenarios can be modeled using the Partially Observable MDP (or POMDP) framework.

A POMDP is a tuple 

, where the first 5 elements are the same as in an MDP, and the last 2 elements are:

Observation Space, 

 : the set of all possible observations that the agent can make.

Observation Probabilities, 

 : a function that gives the probability of making an observation given a specific state.

As in the MDP, the agent's goal in a POMDP is to find the policy that maximizes the expected cumulative reward over time. However, in a POMDP, the agent must also consider the probability of making an observation given the state and action, and the probability of being in a state given the observation and action. The agent must then use these probabilities to update its belief about which state it is in, and then use this updated belief to compute the expected value of each state.

To build more intuition into what these observations and observations probabilities are, let us go back to the following example from class, where a robot tries to navigate a grid-world, but without information about where it is in the grid. Instead, the robot has a set of four sensors, that allow it to see whether there is an empty cell in each of the four cardinal directions. The colored cells denote obstacles in the image below.

From any cell, an observation is simply what the robot's sensors return; an observation in this setting, therefore, is a binary vector of length 4, where each entry tells us whether there is an empty cell in that direction. Notationally, we will be following the order [UP, DN, LT, RT] to denote an observation. Consider the current shown position of the robot, i.e., the cell (3,0). The robot senses an obstacle/wall in the UP, DN and LT directions, but a free cell in the RT direction. The observation 

 therefore is the binary vector [0,0,0,1].

Now consider a scenario where the robot's sensors may be faulty, and return an incorrect reading (i.e., a flipped bit) with some probability 

. For now, let us assume the probability of a faulty reading from a given sensor is 

. In this case, from a given cell, the agent may get more than one possible observation, as a result of one or more faulty sensor readings. Consider the same cell (3,0) as above. If all sensors work correctly, then the observation is [0,0,0,1]. However, if only the UP sensor fails, then the agent observes [1,0,0,1] instead. If instead, only the RT sensor fails, then we get [0,0,0,0]. If the UP and LT sensors fail, we get [1,0,1,1]. Similarly, through various combinations of sensor failures, the agent can observe all possible binary vectors of length 4 from each cell in the grid. However, not all of these observations are equally likely! For instance, to get the observation[1,1,1,0] from the cell (3,0), all four sensors on the robot would have to fail simultaneously! The probability of this happening is 

. In other words, the probability of getting the observation [1,1,1,0] from the cell (3,0) is 

. Similarly, the probability of observing [1,0,1,1] from cell (3,0) is 

, since two sensors must fail and two others must function correctly to return this specific observation from this cell. These calcualtions are nothing but the observation probability 

.

Now, given an observation, the agent must update its belief about which state it is actually in, since the agent does not know this for sure. The belief state is defined as a probability distribution over the state space, representing the agent's likelihood of being in each state in the state-space. Since actions are still stochastic, instead of moving from state 

 to 

, in a POMDP, the agent transitions between belief states, say 

 to 

 as the result of an action 

. This is known as the belief-state approach to solving POMDPs. To build intuition about this, let us consider the same grid-world POMDP as above. Let us assume the agent is in the (3,0) cell as before (although the agent does not know this), and makes the observation [0,0,0,1]. Now, we first need to compute the probability of the agent being in every cell in the grid-world, given this observation. Luckily, Bayes' theorem allows us to invert this dependency, and compute the probability as follows:

where 

 is the probability of making the observation 

 given the state 

 - this is nothing but 

, 

 is the prior probability of being in state 

, and 

 is the probability of making the observation 

. In the grid-world example, the prior probability of being in any cell at the beginning of time is 

, since the agent has no information about its initial position and could be in any state. The denominator 

 is the probability of making the observation 

 from any state, and is given by:

In practice, we can usually ignore the denominator, since it is the same for all states in the equation for 

, and only serves to normalize the belief state. Therefore, we can compute the belief state as:

where 

 is the probability of being in state 

 given the observation 

. In the grid-world example, the belief state after making the observation [0,0,0,1] from the cell (3,0) is given by:

This is computed for each cell 

 in the grid world, and after normalization, results in the following probability distribution:

Now, assume the agent takes an action, say, a step towards the right, and receives a new observation, [1,0,0,1]. We can now begin to reason about the transition between belief states for POMDPs, instead of the transition model between states in standard MDPs. First, let us compute the belief state update resulting from this specific action and observation, and we will then discuss the belief state transition in general. The belief state update is given by:

where 

 is the probability of being in state 

 after taking action 

 and making observation 

, and 

 is the last-known probability of being in state 

 before taking action 

. In the grid-world example, the belief state update after taking the action "right" and making the observation [1,0,0,1] from the cell (3,0) is given by:

There are a few key things to keep in mind here. First, note that we update the probability of being in every target cell 

, summing over all origin cells 

 from where the action 

 could have brought us to 

. This is opposite to the MDP Bellman equation, where we update the value of every origin state 

, summing over all target states 

 under the action 

. Second, just like the Bellman updates, the above belief-state update is calculated for every state in the state-space, considering each as a target state under the action 

. Third, actions are stochastic, therefore we still have a notion of an action taking the agent to a different state than intended. In this example, let us assume that an action fails with probability 

. If the action fails at a state where the only possible action is the valid action (such as the bottom left cell in the grid, and the 'right' action), then the agent stays in the same cell with probability 

 and moves in the correct direction with probability 

. If the action fails at a state where the agent can move in a different direction than intended (such as from the cell (3,1) under the 'right' action), then it moves in the correct direction with probability 

, and one of the other directions (in this case, up or left) with probability 

 each.

The complete calculation for the belief state update for all cells is best automated with code, but we show the update for one particular cell, say (3,1) as an example below:

Now, we can plug in some known values into this equation. First, note that the possible states from which the agent could have reached 

 under the 'right' action are its three neighbors, since actions are stochastic. Therefore, the sum over 

 is essentially a sum over the set of states 

. Next, we note that 

 is independent of 

, and can be computed as we did earlier. The observation at 

 if all sensors work correctly should be 

; thus, in order to observe 

, sensor 3 must have failed, and all others worked correctly. The probability of this occurrence is 

. Next, the transition probabilities 

 are the same as in the MDP, and can be computed from the rules of the grid-world. For instance, 

, 

, since the agent can only slip into 

 if the action fails from 

 by design, and 

, since the agent can slip either up or down from the cell 

. Finally, the last-known belief state 

 is simply the set of values in the figure above, and we can substitute these into our update equation:

Similarly, we can update the belief state probabilities for every cell in our grid-world, and then normalize the belief state to get the final belief state after the action and observation. The original belief state, and the updated belief state after the action "right" and observation [1,0,0,1] are shown below:

Green cells in each distribution denote the states with the highest probabilities, and red cells denote the states with the lowest probabilities. Note how the distribution of the agent's belief about its position has changed after the action and observation; specifically, the probability mass has shifted to the right, since the agent has made an observation that is somewhat consistent with the action it took.

The final step in our reasoning now has to do with the transitions between belief states, conditioned on an action. This is the POMDP equivalent of the transition model 

 in MDPs. In the case of POMDPs, we are looking for a transition model between belief states, i.e., 

. Note that this quantity is slightly different from the belief state updates we have done so far - we are now trying to estimate the probability of transitioning to one specific belief state (or probability distribution) from a previous one, given an action, accounting for all possible observations as a result of that action. We must keep in mind that unlike the number of states in an MDP, the number of belief states in a POMDP is infinite, since every probability distribution over the state-space is a valid belief state. However, given any one belief state, if we take an action and make an observation, we can only end up in one new belief state (just like what we did earlier, the two belief states in the above figure can be thought of as 

 and 

 respectively).

To build intuition into this, let us consider the same grid-world example, and start by computing the probability of transitioning to a specific belief state from a previous belief state, given an action and observation. For instance, let us consider the transition from the belief state shown in the first figure above to the belief state shown in the second figure, given the action "right" and observation [1,0,0,1]. This transition is given by:

Since the agent transitioned into some state 

 as the result of the action 

 (despite our uncertainty), we can compute this transition by accouting for all possible transitions 

 to 

. Since we are now not given an observation, we must also account for all possible observations 

, and therefore sum over 

 as well:

This is the probability of transitioning to the belief state 

, given the action "right" and the previous belief state 

. By summing over all 

, we account for every possible observation the agent may receive (doing this takes care of faulty sensors for instance), and summing over 

 accounts for every state the agent might end up in, starting from some state 

 as defined by our previous belief state. We can expand this as follows using the chain rule of conditional probability:

Now note that 

 is the probability of a specific belief state, given the previous belief state, an action, an observation, and the new state 

. However, since 

 itself is a distribution over all 

, we can drop that dependency from the conditional. Similarly 

 is the probability of making the observation 

 given the state 

 and the action 

. However, observations only depend on 

, hence we can drop the dependency on 

 and 

, and this term turns out to be nothing but 

. This reduces our equation to:

Finally, 

 is the probability of transitioning to state 

 from the previous belief state 

 under the action 

. To calculate this, we simply need to multiply each transition probability 

 by the probability that the agent was indeed in state 

, which is nothing but 

; and sum over all states 

, since we are uncertain about the agent's actual position. Our transition equation finally becomes:

We can now plug in known values for the transition probabilities, observation probabilities, and the previously computed belief state. However we still need to deal with the first term, 

. Now recall that this is the probability that the agent was in belief state 

, took the 'right' action, observed 

 and ended up in 

. From our previous belief state update (shown in the figure with two distributions above), we know that from a particular 

 (say the distribution on the left), for a particular action and observation, we transition to exactly one new belief state 

 (the distribution on the right). Therefore, 

 if the action and observation indeed lead to the new belief state under consideration, and 

 otherwise.

We now have the following quantities, using which a POMDP can be represented as an MDP over belief states:

Space of Belief States, 

 : probability distributions over the state space, representing the agent's likelihood of being in each state in the state-space.

Action Space, 

 : Same as MDPs.

Reward Function, 

 : the expected reward of transitioning from a previous belief state to a new belief state, given an action.

Belief State Transition Model, 

 : the probability of transitioning to a specific belief state from a previous belief state, given an action and observation.

Discount Factor, 

 : same as MDPs.

For further reading on tractable methods to solve POMDPs, please refer to POMDP.org - a POMDP tutorial.
